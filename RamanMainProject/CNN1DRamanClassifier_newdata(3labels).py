# -*- coding: utf-8 -*-
"""conv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FmPveI6oXlUvqpm1hMCtlEl_En7fglwF
"""
import os
import numpy as np
import matplotlib.pyplot as plt
import Copy_files as Cf
import tensorflow as tf
from tensorflow import keras
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.layers import Dense, Input, Conv1D, Flatten, MaxPooling1D, Dropout
from keras.callbacks import TensorBoard
import CSVCSVReader
import pandas as pd
import random

def save_graph(location, file_name):
    plt.savefig(location+'/'+ file_name)
def plot_history(history, fold_no):
    save_location= 'C:/Users/trg14/Documents/Files học tập/Files học tập/Nghiên cứu Raman trong việc phát hiện bệnh tiểu đường/Training Datas in 16-01-2024/3_labels/truong_4'
    plt.figure()
    plt.plot(history.history['loss'], label='Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss for {data_file_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    save_graph(save_location, 'loss' + str(fold_no))
    plt.figure()
    plt.plot(history.history['accuracy'], label='Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'Accuracy for {data_file_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    save_graph(save_location, 'acc' + str(fold_no))
def plot_model_loss(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper right')
    plt.show()
def shuffle_data(data, label):
    print('Data shape: ', data.shape)
    print('Label shape: ', label.shape)

    old_index = list(range(len(data)))

    print('Old order: ', old_index)
    print('Old label: ', label)
    print('Old first data: ', data[0])

    new_index = old_index.copy()
    random.shuffle(new_index)
    new_data = [data[i] for i in new_index]
    new_label = [label[i] for i in new_index]

    print('New order: ', new_index)
    print('New label: ', new_label)
    print('New first data: ', new_data[0])
    return new_data, new_label, new_index
def listfile(source_link, folder_name):
    n = 0
    INPUT = source_link + folder_name
    filenames = []

    for filename in os.listdir(INPUT):
        n += 1
        filenames.append(filename)
        print(filename)
    return filenames
def loaddata(source_link, folder_name):
    X = []
    listf = listfile(source_link, folder_name)
    for i in range(0, len(listf)):
        data_array = CSVCSVReader.get_intensity_data(source_link + folder_name + '/', listf[i])
        X.insert(i, data_array)
    return X

source_folder = '16-1-2024/datas'
destination_folder = '16-1-2024/datas'
label_folder = '16-1-2024/2_datas_labels_truong'
data_folder = '16-1-2024/2_datas_labels_truong'
data_file_name = 'datas.csv'
label_file_name = 'labels.csv'

fold = 100
accuracy = []
val_accuracy = []

raw_data = pd.read_csv('16-1-2024/3_datas_labels_truong/datas.csv', header=None)
raw_data = raw_data.to_numpy()
raw_labels = pd.read_csv('16-1-2024/3_datas_labels_truong/labels.csv')['has_DM2']
raw_labels = np.array([int(x) for x in raw_labels])

print(raw_data)
print(raw_labels)

data, target, new_labels_order = shuffle_data(raw_data, raw_labels)
data = np.array(data)
target = np.array(target)

max_val_acc = []

for i in range(fold):
    sample_size = data.shape[0]         # number of samples in train set
    time_steps = data.shape[1]         # number of features in train set
    input_dimension = 1                 # each feature is represented by 1 number

    data_reshaped = data.reshape(sample_size, time_steps, input_dimension)
    print("After reshape train data shape:\n", data_reshaped.shape)
    print("1 Sample shape:\n",data_reshaped[0].shape)
    print("An example sample:\n", data_reshaped[0])

    # target_reshaped = target.reshape(target.shape[0],1,1)
    target_reshaped = keras.utils.to_categorical(target)
    print("After reshape train target shape:\n", target_reshaped.shape)
    print("1 Sample shape:\n", target_reshaped[0].shape)
    print("An example sample:\n", target_reshaped[0])

    tensorboard = TensorBoard(
      log_dir='.\logs',
      histogram_freq=1,
      write_images=True
    )
    keras_callbacks = [
      tensorboard
    ]

    # loss_function = tf.keras.losses.BinaryCrossentropy()
    loss_function = tf.keras.losses.CategoricalCrossentropy()
    optimizer = Adam(learning_rate=1e-4)
    early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=0, mode='auto', restore_best_weights=True)
    batch_size = 8
    no_epochs = 200

    n_timesteps = data_reshaped.shape[1] #10
    print(n_timesteps)

    n_features = data_reshaped.shape[2] #1
    print(n_features)

    model = keras.Sequential(name="model_conv1D")
    model.add(Input(shape=(n_timesteps, n_features)))
    # model.add(Conv1D(filters=400, kernel_size=48, activation='relu', name="Conv1D_1", kernel_regularizer=l2(0.05)))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=150, kernel_size=12, activation='relu', name="Conv1D_4", kernel_regularizer=l2(0.05)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=50, kernel_size=6, activation='relu', name="Conv1D_5", kernel_regularizer=l2(0.05)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=25, kernel_size=3, activation='relu', name="Conv1D_6", kernel_regularizer=l2(0.05)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(100, activation='relu', name="Dense_1"))
    model.add(Dropout(0.4))
    model.add(Dense(50, activation='relu', name="Dense_2"))
    model.add(Dropout(0.3))
    # model.add(Dense(1, activation='sigmoid', name="Sigmoid"))
    model.add(Dense(3, activation='Softmax', name="Softmax"))
    # Compile the model
    model.compile(loss=loss_function,
                  optimizer=optimizer,
                  metrics=['accuracy'])
    model.summary()

    # Fit data to model
    history = model.fit(data_reshaped, target_reshaped,
                batch_size = batch_size,
                epochs = no_epochs,
                verbose = 1,
                validation_split = 0.2,
                callbacks=[early_stopping]
    )
    model.evaluate(data_reshaped, target_reshaped)

    graph = plt.plot(history.history['accuracy'], label='Accuracy')
    accuracy_array = graph[0].get_data()[1]
    graph = plt.plot(history.history['val_accuracy'], label='Val_Accuracy')
    val_accuracy_array = graph[0].get_data()[1]
    print("Average accuracy: ", np.mean(history.history['accuracy']))
    print("Epoch accuracy detail: ")
    print(accuracy_array)
    print("Average validation accuracy: ", np.mean(history.history['val_accuracy']))
    print("Epoch validation accuracy detail: ")
    print(val_accuracy_array)
    max_val_acc.append(max(val_accuracy_array))
    accuracy.append(np.mean(history.history['accuracy']))
    val_accuracy.append(np.mean(history.history['val_accuracy']))
    plot_history(history, i)
print("Average of training accuracy after ", fold, " is: ", np.mean(accuracy))
print("Average accuracy detail: ", accuracy)
print("Average of validation accuracy after ", fold, " is: ", np.mean(val_accuracy))
print("Average validation accuracy detail: ", val_accuracy)
print("Average validation of maximum accuracy of each folds is: ", np.mean(max_val_acc))
print("Maximum validation accuracy of each folds are: ", max_val_acc)
print("The highest validation accuracy is: ", max(max_val_acc))



