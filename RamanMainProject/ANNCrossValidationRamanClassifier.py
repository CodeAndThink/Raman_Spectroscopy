# -*- coding: utf-8 -*-
"""conv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FmPveI6oXlUvqpm1hMCtlEl_En7fglwF
"""

import numpy as np
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn import metrics
import Copy_files as Cf
from sklearn.preprocessing import LabelEncoder
from numpy import mean
from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold
from tensorflow import keras
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.optimizers import Adam
from keras.regularizers import l2
# Import TensorBoard
from keras.callbacks import TensorBoard
from sklearn.metrics import roc_curve, roc_auc_score, recall_score, confusion_matrix

# import MachineLearningCalculator
# from TextWriter import convert_to_array


def plot_accuracy_per_fold():
    plt.plot(acc_per_fold)
    plt.title(f'Accuracy per fold for {data_file_name}')
    plt.xlabel('Fold')
    plt.ylabel('Accuracy')
    plt.legend(['Accuracy'])
    # plt.savefig(f'plot/nn_{file_name}.pdf', bbox_inches='tight')
    plt.show()

def plot_history(history):
    plt.plot(history.history['loss'], label='Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss for {data_file_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(f'plot/loss_{data_file_name}.pdf', bbox_inches='tight')
    plt.show()

    plt.plot(history.history['accuracy'], label='Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'Accuracy for {data_file_name}')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.savefig(f'plot/accuracy_{data_file_name}.pdf', bbox_inches='tight')
    plt.show()

def plot_model_loss(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper right')
    plt.show()

def extract_data_label(data_array, label_array, selected_index_array):
    extracted_data_array = []
    extracted_label_array = []
    for i in range(0,len(selected_index_array)):
        extracted_data_array.append(data_array[selected_index_array[i]])
        extracted_label_array.append(label_array[selected_index_array[i]])
    return extracted_data_array, extracted_label_array


def get_label_code_array(arrays):
    code_array = []
    for array in arrays:
        maxvalue = max(array)
        for i in range(0, len(array)):
            if(array[i] == maxvalue):
                code_array.append(i)
    return code_array

def get_new_labels(y):
    y_new = LabelEncoder().fit_transform([''.join(str(l)) for l in y])
    return y_new

plt.rcParams["figure.figsize"] = [7.50, 3.50]
plt.rcParams["figure.autolayout"] = True

# sourcelink = 'C:/Users/trg14/PycharmProjects/RamanMainProject/ductri_dataset/data'
sourcelink_datas = 'Datas_Until_11-01-2024/Anh Huy/compile_datas'
sourcelink_labels = 'Datas_Until_11-01-2024/Anh Huy/labels'
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport matplotlib.pyplot as plt
import seaborn as sns

# before Noise-added-Data_Argumentation
# data_file_name = 'DataMay2023_14labels_data'
# label_file_name = 'DataMay2023_14labels_labels'

# after Noise-added-Data_Argumentation
# data_file_name = 'DataMay2023_14labels_data-Noise-added-Data_Argumentation'
# label_file_name = 'DataMay2023_14labels_labels-Noise-added-Data_Argumentation'

data_file_name = 'datas'
label_file_name = 'labels'

data = pd.read_csv(f'{sourcelink_datas}/{data_file_name}.csv', header=None)
data = data.to_numpy()

# target = pd.read_csv(sourcelink + '/' + label_file_name + '.csv')['has_DM2']
target = pd.read_csv(sourcelink_labels + '/' + label_file_name + '.csv', header=None)
target = np.array([int(x) for x in target])

# data, target, new_labels_order = Cf.shuffle_data(data, target)
# data = np.array(data)
# target = np.array(target)

# print(earData.shape)
# print(earData[0].shape)
# print(earData[0])

sample_size = data.shape[0]         # number of samples in train set
time_steps = data.shape[1]         # number of features in train set
input_dimension = 1                 # each feature is represented by 1 number

data_reshaped = data.reshape(sample_size, time_steps, input_dimension)
print("After reshape train data shape:\n", data_reshaped.shape)
print("1 Sample shape:\n",data_reshaped[0].shape)
print("An example sample:\n", data_reshaped[0])
# print(data_reshaped)

target_reshaped = keras.utils.to_categorical(target)
print("After reshape train target shape:\n", target_reshaped.shape)
print("1 Sample shape:\n", target_reshaped[0].shape)
print("An example sample:\n", target_reshaped[0])
# print(target_reshaped)

# num_folds = 5
# num_folds = 10
# loss_function = tf.keras.losses.CategoricalCrossentropy()
# optimizer = Adam(learning_rate=1e-4)

loss_function = keras.losses.BinaryCrossentropy()

# optimizer = Adadelta()
early_stopping = EarlyStopping(monitor='val_loss', patience=150, verbose=0, mode='auto', restore_best_weights=True)
batch_size = 8
# batch_size = 70

# no_epochs = 2000
no_epochs = 100
verbosity = 2
acc_per_fold = []
loss_per_fold = []

n_timesteps = data_reshaped.shape[1] #10
print("Number timesteps:\n", n_timesteps)

n_features = data_reshaped.shape[2] #1
print("Number features:\n",n_features)

# with open("logfile/ann-cross-validation.txt", "w") as file:
#     file.write("Number timesteps: " + str(n_timesteps) + "\n")
#     file.write("Number features: " + str(n_features) + "\n")


# Define the K-fold Cross Validator
num_folds = 5
# kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

num_repeated = 10
kfold = RepeatedStratifiedKFold(n_splits=num_folds, n_repeats=num_repeated)

y_new = get_new_labels(target_reshaped)

# K-fold Cross Validation model evaluation
fold_no = 1

# for train, test in kfold.split(data_reshaped, y_new):
for train, test in kfold.split(data_reshaped, target_reshaped):
    model = keras.Sequential(name="model_conv1D")
    model.add(keras.layers.Input(shape=(n_timesteps, n_features)))
    #model.add(keras.layers.Conv1D(filters=576, kernel_size=24, activation='relu', name="Conv1D_1", kernel_regularizer=l2(0.05)))
    model.add(keras.layers.Conv1D(filters=576, kernel_size=24, activation='relu', name="Conv1D_1", kernel_regularizer=l2(0.05)))
    # model.add(Dropout(0.2))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(keras.layers.Conv1D(filters=144, kernel_size=12, activation='relu', name="Conv1D_2", kernel_regularizer=l2(0.05)))
    # model.add(Dropout(0.2))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(keras.layers.Conv1D(filters=36, kernel_size=6, activation='relu', name="Conv1D_3", kernel_regularizer=l2(0.05)))
    # model.add(Dropout(0.2))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', name="Conv1D_4", kernel_regularizer=l2(0.05)))
    # model.add(Dropout(0.2))
    # model.add(MaxPooling1D(pool_size=2))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(100, activation='relu', name="Dense_2"))
    # model.add(keras.layers.Dense(14, activation='softmax', name="Softmax"))
    model.add(keras.layers.Dense(1, activation='sigmoid', name="Sigmoid"))

    optimizer = Adam(learning_rate=1e-4)

    # Build the optimizer with all trainable variables
    optimizer.build(model.trainable_variables)

    # Compile the model
    model.compile(loss=loss_function,
                  optimizer=optimizer,
                  metrics=['accuracy'])

    # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training on fold {fold_no} ...')
    print(f'Train dataset index {train} ...')
    print(f'Test dataset index {test} ...')

    # with open("logfile/ann-cross-validation.txt", "a") as file:
    #     file.write("Training fold: "+ str(fold_no) +"\n")
    #     file.write("Train dataset index: "+ convert_to_array(train) +"\n")
    #     file.write("Test dataset index: " + convert_to_array(test) + "\n")
    # data_train, label_train = extract_data_label(data_reshaped,target_reshaped, train)
    # Fit data to model
    history = model.fit(data_reshaped[train], target_reshaped[train],
                        batch_size=batch_size,
                        epochs=no_epochs,
                        verbose = verbosity,
                        validation_split=0.2,
                        callbacks = EarlyStopping(monitor='val_loss',
                                                  patience=300, verbose=2,
                                                  mode='auto',
                                                  restore_best_weights=True))
    # Generate generalization metrics
    scores = model.evaluate(data_reshaped[test], target_reshaped[test], verbose=0)
    y_binary_pred = model.predict(data_reshaped[test])
    y_pred = get_label_code_array(y_binary_pred)
    print("Prediction: ",y_pred)

    y_true = get_label_code_array(target_reshaped[test])
    print("Answer: ",y_true)

    accuracy_score = metrics.accuracy_score(y_pred, y_true)
    print(f'Accuracy = {accuracy_score}')

    # one_vs_rest_sensitivity = MachineLearningCalculator.RvO_sensitivity(y_pred, y_true)
    # print(f'Sensitivity = {one_vs_rest_sensitivity}')
    #
    # one_vs_rest_specificity = MachineLearningCalculator.RvO_specificity(y_pred, y_true)
    # print(f'Specificity = {one_vs_rest_specificity}')
    #
    # with open("logfile/ann-cross-validation.txt", "a") as file:
    #     file.write("Prediction: "+ convert_to_array(y_pred) +"\n")
    #     file.write("Answer: "+ convert_to_array(y_true) +"\n")
    #     file.write("Accuracy: " + str(accuracy_score) + "\n")
    #     file.write("Sensitivity: " + str(one_vs_rest_sensitivity) + "\n")
    #     file.write("Specificity: " + str(one_vs_rest_specificity) + "\n")

    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}: {model.metrics_names[1]} of {scores[1] * 100} %')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])
    # Increase fold number
    fold_no = fold_no + 1

model.summary()

print(acc_per_fold)
print(mean(acc_per_fold))

cross_validation_scores = pd.DataFrame(
    {'Accuracy': acc_per_fold,
     'Loss': loss_per_fold})
cross_validation_scores.describe()

model.predict(data_reshaped)