# -*- coding: utf-8 -*-
"""ann_5fold_vein.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VfSF4nHKKU4r02VhnbEujDmmepWt2RMB
"""

import datetime
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import seaborn as sns
#from sklearn.utils.fixes import loguniform

from tensorflow.keras.regularizers import l2, l1

# Import TensorBoard
from tensorflow.keras.callbacks import TensorBoard
import numpy as np # linear algebra
from numpy import mean
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
#from sklearn.utils.fixes import loguniform
from sklearn.metrics import classification_report

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Dense, Flatten

# Import TensorBoard
from tensorflow.keras.callbacks import TensorBoard


plt.rcParams["figure.figsize"] = [7.50, 3.50]
plt.rcParams["figure.autolayout"] = True

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport matplotlib.pyplot as plt

sourcelink = 'C:/Users/trg14/PycharmProjects/RamanMainProject/ductri_dataset/'
#sourcelink = 'D:/CÔNG VIỆC/TIẾN SĨ/CÔNG VIỆC/2023 - 03 Áp dụng hotpot series extraction vào đoán đường huyết bằng Raman/Mau du lieu/Anh Duc data/'



file_name = 'earData'

earData = pd.read_csv(f'{sourcelink}matlab_3/{file_name}.csv', header=None)
#earData = pd.read_csv(f'{sourcelink}data/DataMay2023_14labels_data.csv', header=None)

earData = earData.to_numpy()

i = 0

target = pd.read_csv(f'{sourcelink}matlab_raman_preprocessed copy/target.csv')['has_DM2']
#target = pd.read_csv(f'{sourcelink}data/DataMay2023_14labels_labels.csv')['has_DM2']
target = np.array([int(x) for x in target])
plt.stem(target)

def plot_model_loss(history):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper right')
    plt.show()


# Define Tensorboard as a Keras callback
tensorboard = TensorBoard(
  log_dir='.\logs',
  histogram_freq=1,
  write_images=True
)
keras_callbacks = [
  tensorboard
]

sample_size = earData.shape[0] # number of samples in train set
time_steps  = earData.shape[1] # number of features in train set
input_dimension = 1               # each feature is represented by 1 number

earData_reshaped = earData.reshape(sample_size,time_steps,input_dimension)
print("After reshape train data set shape:\n", earData_reshaped.shape)
print("1 Sample shape:\n",earData_reshaped[0].shape)
print("An example sample:\n", earData_reshaped[0])

target_reshaped = target.reshape(target.shape[0],1,1)
print("After reshape test data set shape:\n", target_reshaped.shape)
print("1 Sample shape:\n",target_reshaped[0].shape)
print("An example sample:\n", target_reshaped[0])

num_folds = 5
#loss_function = tf.keras.losses.CategoricalCrossentropy()
loss_function = tf.keras.losses.BinaryCrossentropy()
optimizer = Adam(learning_rate=3e-4)
batch_size = 8
no_epochs = 100
verbosity = 1
acc_per_fold = []
loss_per_fold = []
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

n_timesteps = earData_reshaped.shape[1] #13
n_features  = earData_reshaped.shape[2] #1 


# Define the K-fold Cross Validator
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# K-fold Cross Validation model evaluation
fold_no = 1
for train, test in kfold.split(earData, target):

  # print(train)

  # Define the model architecture
  model = Sequential()
  model.add(Flatten(input_shape=(n_timesteps, n_features)))
  model.add(Dense(10, activation='tanh', kernel_regularizer=l2(0.05)))
  # model.add(Dropout(0.8))
  model.add(Dense(5, activation='relu'))
  # model.add(Dropout(0.8))
  model.add(Dense(1, activation='sigmoid'))

  # Compile the model
  model.compile(loss=loss_function,
                optimizer=optimizer,
                metrics=['accuracy'])


  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training on fold {fold_no} ...')

  # Fit data to model

  history = model.fit(earData, target,
              batch_size=batch_size,
              epochs=no_epochs,
              verbose=verbosity,
              validation_split=0.2,
              callbacks=EarlyStopping(monitor='val_loss', patience=300, verbose=2, mode='auto', restore_best_weights=True))

  # Generate generalization metrics
  scores = model.evaluate(earData[test], target[test], verbose=0)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}: {model.metrics_names[1]} of {scores[1]*100} %')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  fold_no = fold_no + 1

model.summary()

def plot_history(history):
    plt.plot(history.history['loss'], label='Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Learning curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(f'plot/loss_{file_name}_{datetime.datetime.now()}.pdf', bbox_inches='tight')
    plt.show()


print(f"acc_per_fold: {acc_per_fold}")
print(f"mean acc per fold: {mean(acc_per_fold)}")

cross_validation_scores = pd.DataFrame(
    {'Accuracy': acc_per_fold,
        'Loss': loss_per_fold})
print(cross_validation_scores.describe())

model.predict(earData)
